{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Toolbox for Common Tasks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I find that many of the tasks I perform in my work often employ similar components which could be generalized for more efficient use.  This script will serve to build a package that I can easily access so I don't keep reinventing the wheel."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Basic data manipulation\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from pandas import Series, DataFrame\n",
      "\n",
      "#API Calls\n",
      "import pandas.io.data as web\n",
      "import urllib2\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/choct155/analysis/Anaconda/lib/python2.7/site-packages/pytz/__init__.py:35: UserWarning: Module argparse was already imported from /home/choct155/analysis/Anaconda/lib/python2.7/argparse.pyc, but /home/choct155/analysis/Anaconda/lib/python2.7/site-packages is being added to sys.path\n",
        "  from pkg_resources import resource_stream\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Check 1-to-1 Correspondence"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def one2one(seq1,seq2):\n",
      "    '''Function checks for 1-to-1 correspondence in two sequences'''\n",
      "    #Create dictionary from sequences\n",
      "    seq_dict=dict(zip(seq1,seq2))\n",
      "    \n",
      "    #Create container for values\n",
      "    values=[]\n",
      "    \n",
      "    #For each key in the dictionary\n",
      "    for key in seq_dict:\n",
      "        #...add the corresponding value to the list\n",
      "        values.append(seq_dict[key])\n",
      "    \n",
      "    #Evaluate whether or not the set of matched values is the same as the unique values of the original seq2\n",
      "    return set(values)==len(set(seq2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Inflation Adjustment"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def defl(yr0,yrn,base):\n",
      "    '''Function takes the start, end and base years as inputs and returns a Series of deflators.'''\n",
      "    #Capture data from FRED\n",
      "    cpi=web.get_data_fred('USACPIBLS','1/1/'+str(yr0),'1/1/'+str(yrn)).rename(columns={'USACPIBLS':'cpi'})\n",
      "    \n",
      "    #Capture year from index\n",
      "    cpi['year']=cpi.index.year\n",
      "    \n",
      "    #Set year as index\n",
      "    cpi.set_index('year',inplace=True)\n",
      "    \n",
      "    #Calculate deflators\n",
      "    cpi['defl']=cpi.div(cpi.ix[base])\n",
      "    \n",
      "    return cpi\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Census API Call"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def census(year,product,indicator,geo,location,prnt=False):\n",
      "    #Establish request string\n",
      "    req='http://api.census.gov/data/'+year+'/'+product+'?key=b62798704de5e04d638987171ec291a6889c16bf&get='+indicator+',NAME&for='+geo+'&in='+location\n",
      "    \n",
      "    #Print option\n",
      "    if prnt:\n",
      "        print req\n",
      "    else:\n",
      "        pass\n",
      "    #Open connection to Census API\n",
      "    request = urllib2.urlopen(req)\n",
      "    \n",
      "    #Read data as json and use convert to dataframe\n",
      "    req = pd.read_json(request.read())\n",
      "    \n",
      "    #First row of 'data' values are really column headings, so subset to real data\n",
      "    req_df=DataFrame(req[1:])\n",
      "    \n",
      "    #Use that first row to set column labels\n",
      "    req_df.columns=list(req[:1].values)\n",
      "    return req_df\n",
      "\n",
      "def subcty(cty_list,year,product,indicator,geo,location,cty_name):\n",
      "    #Generate container for county level data sets\n",
      "    cty_data=[]\n",
      "    \n",
      "    #For each county ...\n",
      "    for cty in cty_list:\n",
      "        #Create a new, composite location parameter\n",
      "        loc_cty=location+'+county:'+cty\n",
      "        \n",
      "        #Use inputs from warapper function and substitute location\n",
      "        data=census(year,product,indicator,geo,loc_cty)\n",
      "        \n",
      "        #Throw county level result in container\n",
      "        cty_data.append(data)\n",
      "    \n",
      "    #Concatenate into one set after all county data is read in\n",
      "    all_cty=pd.concat(cty_data)\n",
      "    \n",
      "    #Include count names\n",
      "    all_cty['cty_name']=all_cty['county'].map(cty_name)\n",
      "    \n",
      "    return all_cty"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Shapefile to DataFrame"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Define function that reads shapefile info and deposits it into a DF\n",
      "def shp2df(f):\n",
      "    '''Function takes a shapefile as an input and returns a DF with all attribute information and a 'geometry'\n",
      "    variable that houses the spatial elements in the first position.  In the second position the metadata is\n",
      "    returned.'''\n",
      "    #Open shapefile\n",
      "    shps=fiona.open(f)\n",
      "    \n",
      "    #Create container for attribute data\n",
      "    attr_data=[]\n",
      "    \n",
      "    #Create container for shapes\n",
      "    shape_list=[]\n",
      "    \n",
      "    #For each feature...\n",
      "    for shp in shps:\n",
      "        #...create a field container for feature-specific attributes...\n",
      "        field_list=[]\n",
      "        #...and for each field...\n",
      "        for field in shp['properties']:\n",
      "            #...capture each field value in the field list...\n",
      "            field_list.append(shp['properties'][field])\n",
      "        #...then dump the record in the attribute list...\n",
      "        attr_data.append(field_list)\n",
      "        #...and capture the associated shape\n",
      "        shape_list.append(shape(shp['geometry']))\n",
      "        \n",
      "    #Capture field labels\n",
      "    field_labels=shps.meta['schema']['properties'].keys()\n",
      "    \n",
      "    \n",
      "    #Construct DF from attribute data\n",
      "    sp_df=pd.DataFrame(attr_data,columns=field_labels)\n",
      "    \n",
      "    #Integrate shapes\n",
      "    sp_df['geom_in']=pd.Series(shape_list,index=sp_df.index)\n",
      "    \n",
      "    return [sp_df,shps.meta]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}