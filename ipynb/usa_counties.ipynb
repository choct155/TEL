{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "CenStats"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I have started gathering the economic base with [County Business Patterns](http://www.census.gov/econ/cbp/), but it quickly became apparent that mapping at the detailed level was ... problematic.  It's not that it can't be done, it's that it can't be done cleanly.  It's apparently easier to get a sense of the loss associated with different mapping rules when one looks at a higher level of aggregation.  \n",
      "\n",
      "As a consequence, I am moving to [USA Counties](http://censtats.census.gov/usa/usa.shtml) to see if I can gain insight, or better yet, simply use the available data.  This is, thus, an exploration of the available data from USA Counties.  The first portion of this script will be a one-time download of the data. From there, we will piece together what we want to merge with the fiscal data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup as bs\n",
      "import os\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "base='http://www.census.gov/support/USACdataDownloads.html'\n",
      "soup=bs(urllib2.urlopen(base))\n",
      "\n",
      "#print soup.prettify()\n",
      "\n",
      "for link in soup.find_all('a'):\n",
      "    #print link.string\n",
      "\n",
      "    if unicode('.xls') in unicode(link.string):\n",
      "        print link.string\n",
      "        print link['href']\n",
      "\"\"\"\n",
      "def costat_fetch(outdir):\n",
      "    \"\"\"Grabs all CoStat spreadsheets and downloads it to outdir\"\"\"\n",
      "    \n",
      "    #Construct URL for year-specific historical data page\n",
      "    base='http://www.census.gov/support/USACdataDownloads.html'\n",
      "    \n",
      "    #Parse contents of year-specific historical data page\n",
      "    soup=bs(urllib2.urlopen(base))\n",
      "    \n",
      "    #Loop through all links\n",
      "    for link in soup.find_all('a'):\n",
      "        #If the link text matches the target...\n",
      "        if unicode('.xls') in unicode(link.string):\n",
      "            #Verify download link\n",
      "            print link['href']\n",
      "            #Establish output path\n",
      "            out=os.path.join(outdir,link.string)\n",
      "            #...extract the href attribute (associated URL) and open socket\n",
      "            response=urllib2.urlopen(link['href'])\n",
      "            #Write contents to disk\n",
      "            open(out,'wb').write(response.read())\n",
      "\n",
      "#costat_fetch('/home/choct155/data/costats')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}