Introduction to Classification with the ISLR Library
========================================================

One of the biggest virtues of the Statistical Learning approach is tight intuitive integration of regression and classification.  To some extent, this has always been partially the case in standard econometric instruction given the prevalence of logit and probit modeling.  To my mind, however, these models were offered as ways to deal with categorical data, but beyond the linear form no strong fundamental connection was pursued *in my experience*.  In the Statistical Learning approach, heavy emphasis is made on the idea that we are always trying to estimate parameters based upon what is going on in the local neighborhood of the observation.  From this basis, regression and classification are natural extensions.

This script will work through some examples of the latter.

```{r Library Load}
library(ISLR)
```

For this task we will employ a dataset called **`Smarket`**.

```{r Data Explor}
#View variable names
names(Smarket)
#View data summary
summary(Smarket)
#View correlation matrix
pairs(Smarket,col=Smarket$Direction)
```

Looks like these variables are fairly uncorrelated.  Let's examine our first model.

```{r Logistic Regression}
#Fit a logistic regression model (indicated by family=binomial)
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
            data=Smarket,family=binomial)
#View model summary
summary(glm.fit)
```

Interestingly enough, even though correlation across the variables does not appear to be an issue, we have remarkably low significance for all considered variables.  We can use the fitted model to predict the `Direction` of new observations.

```{r Prediction}
#Generate prediction object
glm.probs=predict(glm.fit,type='response')
#Predict the probability of Direction='Up' for the first five observations in the training set
glm.probs[1:5]
#Turn these predictions into classifications
glm.pred=ifelse(glm.probs>.5,'Up','Down')
#View classifications
table(glm.pred,Smarket$Direction)
#View mean performance (correct matches are worth 1, incorrect matches are 0)
mean(glm.pred==Smarket$Direction)
```

All in all, this models isn't very good.  It makes many mistakes and performs only slightly better than just using the mean value for all predictions (meaning we just assign the majority class to everything).  Nevertheless, let's create separate training and test sets.

```{r Out of Sample Testing}
#Set scope of training data
train=Smarket$Year<2005
#Fit training data
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
            data=Smarket,family=binomial,subset=train)
#Evaluate performance on test data
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type='response')
#Turn probabilities into classifications
glm.pred=ifelse(glm.probs>.5,'Up','Down')
#Explicitly define test data to test predictions
Direction.2005=Smarket$Direction[!train]
#Create table of classifications and check mean performance
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
```

Now we are actually doing worse than the null rate, which probably means we are overfitting.  Let's try a smaller model.

```{r Smaller Model}
#Fit training data
glm.fit=glm(Direction~Lag1+Lag2,
            data=Smarket,family=binomial,subset=train)
#Evaluate performance on test data
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type='response')
#Turn probabilities into classifications
glm.pred=ifelse(glm.probs>.5,'Up','Down')
#Explicitly define test data to test predictions
Direction.2005=Smarket$Direction[!train]
#Create table of classifications and check mean performance
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
```

That kicked up performance quite a bit, which supports the overfitting theory.